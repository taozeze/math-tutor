{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithms and Techniques**\n",
    "\n",
    "I will be using Random Forests and XGBoost as classifiers for this problem and comparing their results to the benchmark model described below. Both of these are strong classification algorithms that work well on similar problems.\n",
    "\n",
    "As discussed in the benchmark model section, this is a binary classification problem and the classes in this dataset are imbalanced (77% of the target values are 1 and 23% are 0). Based on this, I want to focus on maximizing the ability of the model to correctly identify observations in the minority class. As a result I will not be using accuracy as the main metric for evaluating model performance. I will report the following metrics:\n",
    "\n",
    "- F1 Score \n",
    "- AUC/ROC\n",
    "\n",
    "Both of these methods are effective in measuring not only the rate at which a model provides correct predictions but the rate at which it can correctly predict the minority class. I will discuss more on the dataset and class imbalance in the benchmark model section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv as csv\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Benchmark Model**\n",
    "\n",
    "I am using a dummy classifier as a benchmark model. The first dataset I use will include only the KCs as features and does not include any information about the problem name, section of the curriculum, or any identifying information about the student such as the student ID. It also does not include any information that would not have been included in the final test set portion of the KDD Challenge, such as the problem times or hints and incorrects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Changing axis bounds</th>\n",
       "      <th>Changing axis intervals</th>\n",
       "      <th>Choose Graphical Refl-v</th>\n",
       "      <th>Choose Graphical a</th>\n",
       "      <th>Choose Graphical h</th>\n",
       "      <th>Choose Graphical k</th>\n",
       "      <th>Convert unit, mixed</th>\n",
       "      <th>Convert unit, multiplier</th>\n",
       "      <th>Convert unit, standard</th>\n",
       "      <th>Correctly placing points</th>\n",
       "      <th>...</th>\n",
       "      <th>PROP</th>\n",
       "      <th>L1F</th>\n",
       "      <th>NOV</th>\n",
       "      <th>PERCENT</th>\n",
       "      <th>IPC</th>\n",
       "      <th>CTA</th>\n",
       "      <th>ES</th>\n",
       "      <th>LINEAR</th>\n",
       "      <th>QUAD</th>\n",
       "      <th>Problem View</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 142 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Changing axis bounds Changing axis intervals Choose Graphical Refl-v  \\\n",
       "0                    0                       0                       0   \n",
       "1                    0                       0                       0   \n",
       "2                    0                       0                       0   \n",
       "3                    0                       0                       0   \n",
       "4                    0                       0                       0   \n",
       "\n",
       "  Choose Graphical a Choose Graphical h Choose Graphical k  \\\n",
       "0                  0                  0                  0   \n",
       "1                  0                  0                  0   \n",
       "2                  0                  0                  0   \n",
       "3                  0                  0                  0   \n",
       "4                  0                  0                  0   \n",
       "\n",
       "  Convert unit, mixed Convert unit, multiplier Convert unit, standard  \\\n",
       "0                   0                        0                      0   \n",
       "1                   0                        0                      0   \n",
       "2                   0                        0                      0   \n",
       "3                   0                        0                      0   \n",
       "4                   0                        0                      0   \n",
       "\n",
       "  Correctly placing points     ...      PROP L1F NOV PERCENT IPC CTA ES  \\\n",
       "0                        0     ...         0   0   0       0   0   0  1   \n",
       "1                        0     ...         0   0   0       0   0   0  1   \n",
       "2                        0     ...         0   0   0       0   0   0  1   \n",
       "3                        0     ...         0   0   0       0   0   0  1   \n",
       "4                        0     ...         0   0   0       0   0   0  1   \n",
       "\n",
       "  LINEAR QUAD Problem View  \n",
       "0      0    0            1  \n",
       "1      0    0            1  \n",
       "2      0    0            1  \n",
       "3      0    0            1  \n",
       "4      0    0            2  \n",
       "\n",
       "[5 rows x 142 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle('mtprocessed.p')\n",
    "#df = df.apply(pd.to_numeric, errors='coerce', axis=1)\n",
    "#df.columns = df.columns.str.replace(\"],[<\", \"\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df['is_train'] = np.random.uniform(0, 1, len(df)) <= .75\n",
    "#train, test = df[df['is_train']==True], df[df['is_train']==False]\n",
    "df = df.drop('Correct First Attempt',1).join(df['Correct First Attempt']) #make CFA the last column\n",
    "\n",
    "# Show the number of observations for the train and test dataframes\n",
    "#print('Number of observations in the training data:', len(train))\n",
    "#print('Number of observations in the test data:', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = df.columns[:-1]\n",
    "target = df.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Correct First Attempt'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score (most frequent): 0.7678144688246795\n",
      "Accuracy score (stratified): 0.6423380009084985\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = DummyClassifier(strategy='most_frequent')\n",
    "clf2 = DummyClassifier()\n",
    "clf.fit(train[features], train[target])\n",
    "clf2.fit(train[features], train[target])\n",
    "y_pred1 = clf.predict(test[features])\n",
    "y_pred2 = clf2.predict(test[features])\n",
    "\n",
    "print (\"Accuracy score (most frequent): {0}\".format(accuracy_score(y_pred1, test[target])))\n",
    "print (\"Accuracy score (stratified): {0}\".format(accuracy_score(y_pred2, test[target])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this data is imbalanced, (about 77% of the observations have target “Correct First Attempt” values of 1 and 23% have target values of 0), this model can achieve an accuracy of 77% by simply guessing 1 for every observation (the \"most_frequent\" strategy). This is not an acceptable solution because we also want to be able to identify negative cases accurately, i.e. students who do not answer questions correctly on the first try. This 77% “success” rate is due to the classes being unbalanced, not to the classifier being useful.\n",
    "\n",
    "Due to the class imbalance, accuracy is not a useful metric for this model, and I will be using the F1 score and AUC/ROC scores as metrics instead. In order to draw a meaningful baseline solution, I am using a dummy model with the default \"stratified\" strategy, which respects the original class frequency of the target. We find the F1 score for this model is approximately 0.23 for the minority class and 0.77 for the majority class, which reflects the balance of the distribution and gives us a solution that matches the dataset better than the more naive \"most frequent\" strategy. The accuracy of this model is lower (about 64%), but the F1 score is improved for the minority class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score (most frequent): [ 0.          0.86865956]\n",
      "F1 score (stratified): [ 0.23237183  0.76685399]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print (\"F1 score (most frequent): {0}\".format(f1_score(test[target], y_pred1, average=None)))\n",
    "print (\"F1 score (stratified): {0}\".format(f1_score(test[target], y_pred2, average=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the \"most frequent\" strategy, (for which we receive a warning that \"F-score is ill-defined\" because the 0 class is never predicted, which causes a divide-by-zero error) the F-score for the 0 class is 0.0 and the F-score for the 1 class is about 0.867. As expected, our recall for the 1 class should be perfect and our precision poor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC/ROC score (most frequent): 0.5\n",
      "AUC/ROC score (stratified): 0.49961370874525235\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print (\"AUC/ROC score (most frequent): {0}\".format(roc_auc_score(test[target], y_pred1)))\n",
    "print (\"AUC/ROC score (stratified): {0}\".format(roc_auc_score(test[target], y_pred2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Implementation**\n",
    "\n",
    "Below I am running the XGBoost and random forests classifiers on the dataset to compare their performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier()\n",
    "clf.fit(train[features], train[target])\n",
    "y_pred = clf.predict(test[features])\n",
    "\n",
    "print (\"XGBoost accuracy score: {0}\".format(accuracy_score(y_pred, test[target])))\n",
    "print (\"F1 score: {0}\".format(f1_score(test[target], y_pred, average=None)))\n",
    "print (\"AUC/ROC score: {0}\".format(roc_auc_score(test[target], y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forests accuracy score: 0.7747121442537476\n",
      "F1 score: [ 0.37308675  0.8626829 ]\n",
      "AUC/ROC score: 0.6051971590981953\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train[features], train[target])\n",
    "y_pred = clf.predict(test[features])\n",
    "\n",
    "print (\"Random forests accuracy score: {0}\".format(accuracy_score(y_pred, test[target])))\n",
    "print (\"F1 score: {0}\".format(f1_score(test[target], y_pred, average=None)))\n",
    "print (\"AUC/ROC score: {0}\".format(roc_auc_score(test[target], y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Evaluation and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 607270 samples\n",
      "Test set: 202424 samples\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#df = df.drop('is_train', 1)\n",
    "num_all = len(df)  \n",
    "num_train = int(len(df)*.75) \n",
    "num_test = num_all - num_train\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(df[df.columns[:-1]], df[df.columns[-1]], \n",
    "                                                                     train_size=num_train,\n",
    "                                                                     test_size=num_test, \n",
    "                                                                     random_state=10)\n",
    "\n",
    "\n",
    "print (\"Training set: {} samples\".format(X_train.shape[0]))\n",
    "print (\"Test set: {} samples\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print (\"Random forests accuracy score: {0}\".format(accuracy_score(y_pred, y_test)))\n",
    "print (\"F1 score: {0}\".format(f1_score(y_test, y_pred, average=None)))\n",
    "print (\"AUC/ROC score: {0}\".format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train on smaller training set sizes\n",
    "\n",
    "X_train_10k = X_train[:10000]\n",
    "y_train_10k = y_train[:10000]\n",
    "X_train_100k = X_train[:100000]\n",
    "y_train_100k = y_train[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train_10k, y_train_10k)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print (\"Random forests accuracy score: {0}\".format(accuracy_score(y_pred, y_test)))\n",
    "print (\"F1 score: {0}\".format(f1_score(y_test, y_pred, average=None)))\n",
    "print (\"AUC/ROC score: {0}\".format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train_100k, y_train_100k)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print (\"Random forests accuracy score: {0}\".format(accuracy_score(y_pred, y_test)))\n",
    "print (\"F1 score: {0}\".format(f1_score(y_test, y_pred, average=None)))\n",
    "print (\"AUC/ROC score: {0}\".format(roc_auc_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gridsearchcv on random forests and xgboost\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "parameters = {'max_features':['auto', 'log2'], 'n_estimators':[10, 20, 30]}\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "clf = GridSearchCV(classifier, parameters, scoring=f1_scorer)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "test_f1_score = predict_labels(clf, X_test, y_test)\n",
    "\n",
    "print (\"Optimal parameter values: {}\".format(clf.best_params_))\n",
    "print (\"F1 score for training set: {}\".format(train_f1_score))\n",
    "print (\"F1 score for test set: {}\".format(test_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time \n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    print (\"Predicting labels using {}...\".format(clf.__class__.__name__))\n",
    "    start = time.time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time.time()\n",
    "    print (\"Done!\\nPrediction time (secs): {:.3f}\".format(end - start))\n",
    "    print (f1_score(target, y_pred, average=None))\n",
    "    return f1_score(target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting labels using GridSearchCV...\n",
      "Done!\n",
      "Prediction time (secs): 18.519\n",
      "[ 0.24985872  0.87654126]\n",
      "Predicting labels using GridSearchCV...\n",
      "Done!\n",
      "Prediction time (secs): 4.158\n",
      "[ 0.25264589  0.87665146]\n",
      "Optimal parameter values: {'learning_rate': 0.2, 'subsample': 0.5}\n",
      "F1 score for training set: 0.8765412566317545\n",
      "F1 score for test set: 0.8766514626109729\n"
     ]
    }
   ],
   "source": [
    "#gridsearchcv on random forests and xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "classifier = XGBClassifier()\n",
    "parameters = {'learning_rate':[0.05, 0.1, 0.2], 'subsample':[0.5, 0.8, 1]}\n",
    "f1_scorer = make_scorer(f1_score, pos_label=1)\n",
    "\n",
    "clf = GridSearchCV(classifier, parameters, scoring=f1_scorer)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "train_f1_score = predict_labels(clf, X_train, y_train)\n",
    "test_f1_score = predict_labels(clf, X_test, y_test)\n",
    "\n",
    "print (\"Optimal parameter values: {}\".format(clf.best_params_))\n",
    "print (\"F1 score for training set: {}\".format(train_f1_score))\n",
    "print (\"F1 score for test set: {}\".format(test_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
